{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(layers):\n",
    "    return torch.cat(layers, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomNet(nn.Module):\n",
    "    def init(self,layer_num,channel=64,kernel_size=3):\n",
    "        super(DecomNet, self).init()\n",
    "        self.layer_num = layer_num\n",
    "        self.shallow_feature_extraction=nn.Conv2d(4,channel,kernel_size=kernel_size*3,padding=kernel_size//2)\n",
    "        self.activated_layers = nn.ModuleList([nn.Conv2d(channel,channel,kernel_size=kernel_size,padding=kernel_size//2) for i in range(layer_num)])\n",
    "        self.recon_layers=nn.Conv2d(channel,4,kernel_size=kernel_size,padding=kernel_size//2)\n",
    "    def forward(self,input_im):\n",
    "        input_max, _ = torch.max(input_im, dim=1, keepdim=True)\n",
    "        input_im = concat([input_im, input_max])\n",
    "        conv=self.shallow_feature_extraction(input_im)\n",
    "        for i in range(self.layer_num):\n",
    "            conv = F.relu(self.activated_layers[i](conv))\n",
    "        conv=self.recon_layers(conv)\n",
    "        R=torch.sigmoid(conv[:,0:3])\n",
    "        L=torch.sigmoid(conv[:,3:4])\n",
    "        return R,L\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelightNet(nn.Module):\n",
    "    def __init__(self, channel=64, kernel_size=3):\n",
    "        super(RelightNet, self).__init__()\n",
    "        # Convolutional layers for down-sampling (encoding)\n",
    "        self.conv0 = nn.Conv2d(4, channel, kernel_size, padding=kernel_size // 2)\n",
    "        self.conv1 = nn.Conv2d(channel, channel, kernel_size, stride=2, padding=kernel_size // 2)\n",
    "        self.conv2 = nn.Conv2d(channel, channel, kernel_size, stride=2, padding=kernel_size // 2)\n",
    "        self.conv3 = nn.Conv2d(channel, channel, kernel_size, stride=2, padding=kernel_size // 2)\n",
    "        # Deconvolutional layers for up-sampling (decoding)\n",
    "        self.deconv1 = nn.Conv2d(channel, channel, kernel_size, padding=kernel_size // 2)\n",
    "        self.deconv2 = nn.Conv2d(channel, channel, kernel_size, padding=kernel_size // 2)\n",
    "        self.deconv3 = nn.Conv2d(channel, channel, kernel_size, padding=kernel_size // 2)\n",
    "        # Fusion layer to combine features from different levels\n",
    "        self.feature_fusion = nn.Conv2d(channel * 3, channel, 1, padding=0)\n",
    "        # Output layer to generate the final enhanced illumination map\n",
    "        self.output_layer = nn.Conv2d(channel, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, input_L, input_R):\n",
    "        # Concatenate reflectance map and illumination map\n",
    "        input_im = concat([input_R, input_L])\n",
    "        # Encoding path: apply down-sampling convolutions\n",
    "        conv0 = self.conv0(input_im)\n",
    "        conv1 = F.relu(self.conv1(conv0))\n",
    "        conv2 = F.relu(self.conv2(conv1))\n",
    "        conv3 = F.relu(self.conv3(conv2))\n",
    "        # Decoding path: up-sample and combine with previous layers\n",
    "        up1 = F.interpolate(conv3, size=(conv2.shape[2], conv2.shape[3]), mode='nearest')\n",
    "        deconv1 = F.relu(self.deconv1(up1) + conv2)\n",
    "        up2 = F.interpolate(deconv1, size=(conv1.shape[2], conv1.shape[3]), mode='nearest')\n",
    "        deconv2 = F.relu(self.deconv2(up2) + conv1)\n",
    "        up3 = F.interpolate(deconv2, size=(conv0.shape[2], conv0.shape[3]), mode='nearest')\n",
    "        deconv3 = F.relu(self.deconv3(up3) + conv0)\n",
    "        \n",
    "        # Resize feature maps to match the output size and concatenate\n",
    "        deconv1_resize = F.interpolate(deconv1, size=(deconv3.shape[2], deconv3.shape[3]), mode='nearest')\n",
    "        deconv2_resize = F.interpolate(deconv2, size=(deconv3.shape[2], deconv3.shape[3]), mode='nearest')\n",
    "        feature_gather = concat([deconv1_resize, deconv2_resize, deconv3])\n",
    "        # Fuse features from different levels\n",
    "        feature_fusion = self.feature_fusion(feature_gather)\n",
    "        # Generate the enhanced illumination map\n",
    "        output = self.output_layer(feature_fusion)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
