{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(layers):\n",
    "    return torch.cat(layers, dim=1)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomNet(nn.Module):\n",
    "    def init(self,layer_num,channel=64,kernel_size=3):\n",
    "        super(DecomNet, self).init()\n",
    "        self.layer_num = layer_num\n",
    "        self.shallow_feature_extraction=nn.Conv2d(4,channel,kernel_size=kernel_size*3,padding=kernel_size//2)\n",
    "        self.activated_layers = nn.ModuleList([nn.Conv2d(channel,channel,kernel_size=kernel_size,padding=kernel_size//2) for i in range(layer_num)])\n",
    "        self.recon_layers=nn.Conv2d(channel,4,kernel_size=kernel_size,padding=kernel_size//2)\n",
    "    def forward(self,input_im):\n",
    "        input_max, _ = torch.max(input_im, dim=1, keepdim=True) #1*3*256*256\n",
    "        input_im = concat([input_im, input_max]) #1*4*256*256\n",
    "        conv=self.shallow_feature_extraction(input_im) #1*64*256*256\n",
    "        for i in range(self.layer_num):\n",
    "            conv = F.relu(self.activated_layers[i](conv))\n",
    "        conv=self.recon_layers(conv)\n",
    "        R=torch.sigmoid(conv[:,0:3,:,:])\n",
    "        L=torch.sigmoid(conv[:,3:4,:,:])\n",
    "        return R,L\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelightNet(nn.Module):\n",
    "    def __init__(self, channel=64, kernel_size=3):\n",
    "        super(RelightNet, self).__init__()\n",
    "        # Convolutional layers for down-sampling (encoding)\n",
    "        self.conv0 = nn.Conv2d(4, channel, kernel_size, padding=kernel_size // 2)\n",
    "        self.conv1 = nn.Conv2d(channel, channel, kernel_size, stride=2, padding=kernel_size // 2)\n",
    "        self.conv2 = nn.Conv2d(channel, channel, kernel_size, stride=2, padding=kernel_size // 2)\n",
    "        self.conv3 = nn.Conv2d(channel, channel, kernel_size, stride=2, padding=kernel_size // 2)\n",
    "        # Deconvolutional layers for up-sampling (decoding)\n",
    "        self.deconv1 = nn.Conv2d(channel, channel, kernel_size, padding=kernel_size // 2)\n",
    "        self.deconv2 = nn.Conv2d(channel, channel, kernel_size, padding=kernel_size // 2)\n",
    "        self.deconv3 = nn.Conv2d(channel, channel, kernel_size, padding=kernel_size // 2)\n",
    "        # Fusion layer to combine features from different levels\n",
    "        self.feature_fusion = nn.Conv2d(channel * 3, channel, 1, padding=0)\n",
    "        # Output layer to generate the final enhanced illumination map\n",
    "        self.output_layer = nn.Conv2d(channel, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, input_L, input_R):\n",
    "        # Concatenate reflectance map and illumination map\n",
    "        input_im = concat([input_R, input_L])\n",
    "        # Encoding path: apply down-sampling convolutions\n",
    "        conv0 = self.conv0(input_im)\n",
    "        conv1 = F.relu(self.conv1(conv0))\n",
    "        conv2 = F.relu(self.conv2(conv1))\n",
    "        conv3 = F.relu(self.conv3(conv2))\n",
    "        # Decoding path: up-sample and combine with previous layers\n",
    "        up1 = F.interpolate(conv3, size=(conv2.shape[2], conv2.shape[3]), mode='nearest')\n",
    "        deconv1 = F.relu(self.deconv1(up1) + conv2)\n",
    "        up2 = F.interpolate(deconv1, size=(conv1.shape[2], conv1.shape[3]), mode='nearest')\n",
    "        deconv2 = F.relu(self.deconv2(up2) + conv1)\n",
    "        up3 = F.interpolate(deconv2, size=(conv0.shape[2], conv0.shape[3]), mode='nearest')\n",
    "        deconv3 = F.relu(self.deconv3(up3) + conv0)\n",
    "        \n",
    "        # Resize feature maps to match the output size and concatenate\n",
    "        deconv1_resize = F.interpolate(deconv1, size=(deconv3.shape[2], deconv3.shape[3]), mode='nearest')\n",
    "        deconv2_resize = F.interpolate(deconv2, size=(deconv3.shape[2], deconv3.shape[3]), mode='nearest')\n",
    "        feature_gather = concat([deconv1_resize, deconv2_resize, deconv3])\n",
    "        # Fuse features from different levels\n",
    "        feature_fusion = self.feature_fusion(feature_gather)\n",
    "        # Generate the enhanced illumination map\n",
    "        output = self.output_layer(feature_fusion)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowlightEnhance(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LowlightEnhance, self).__init__()\n",
    "        # Number of layers for DecomNet\n",
    "        self.DecomNet_layer_num = 5\n",
    "        # Initialize DecomNet and RelightNet\n",
    "        self.DecomNet = DecomNet(layer_num=self.DecomNet_layer_num)\n",
    "        self.RelightNet = RelightNet()\n",
    "\n",
    "    def forward(self, input_low, input_high):\n",
    "        # Decomposition\n",
    "        R_low, I_low = self.DecomNet(input_low)\n",
    "        R_high, I_high = self.DecomNet(input_high)\n",
    "\n",
    "        # Relight\n",
    "        I_delta = self.RelightNet(I_low, R_low)\n",
    "\n",
    "        # Concatenate channels\n",
    "        I_low_3 = torch.cat([I_low, I_low, I_low], dim=1)\n",
    "        I_high_3 = torch.cat([I_high, I_high, I_high], dim=1)\n",
    "        I_delta_3 = torch.cat([I_delta, I_delta, I_delta], dim=1)\n",
    "\n",
    "        output_R_low = R_low\n",
    "        output_I_low = I_low_3\n",
    "        output_I_delta = I_delta_3\n",
    "        output_S = R_low * I_delta_3\n",
    "\n",
    "        return output_R_low, output_I_low, output_I_delta, output_S, R_high, I_high_3, I_low, I_high, I_delta\n",
    "\n",
    "    def loss(self, input_low, input_high, output_R_low, output_I_low, output_I_delta, output_S, R_high, I_high_3,I_low, I_high, I_delta):\n",
    "        # Loss calculations\n",
    "        recon_loss_low = torch.mean(torch.abs(output_R_low * output_I_low - input_low))\n",
    "        recon_loss_high = torch.mean(torch.abs(R_high * I_high_3 - input_high))\n",
    "        recon_loss_mutal_low = torch.mean(torch.abs(R_high * output_I_low - input_low))\n",
    "        recon_loss_mutal_high = torch.mean(torch.abs(output_R_low * I_high_3 - input_high))\n",
    "        equal_R_loss = torch.mean(torch.abs(output_R_low - R_high))\n",
    "        relight_loss = torch.mean(torch.abs(output_R_low * output_I_delta - input_high))\n",
    "\n",
    "        # Smoothness loss\n",
    "        Ismooth_loss_low = self.smooth(I_low, output_R_low)\n",
    "        Ismooth_loss_high = self.smooth(I_high, R_high)\n",
    "        Ismooth_loss_delta = self.smooth(I_delta, output_R_low)\n",
    "\n",
    "        # Total losses\n",
    "        loss_Decom = recon_loss_low + recon_loss_high + 0.001 * recon_loss_mutal_low + \\\n",
    "                     0.001 * recon_loss_mutal_high + 0.1 * Ismooth_loss_low + 0.1 * Ismooth_loss_high + \\\n",
    "                     0.01 * equal_R_loss\n",
    "\n",
    "        loss_Relight = relight_loss + 3 * Ismooth_loss_delta\n",
    "\n",
    "        return loss_Decom, loss_Relight\n",
    "\n",
    "    def gradient(self, input_tensor, direction):\n",
    "        smooth_kernel_x = torch.Tensor([[0, 0], [-1, 1]]).unsqueeze(0).unsqueeze(0).to(input_tensor.device)\n",
    "        smooth_kernel_y = smooth_kernel_x.transpose(1, 2)\n",
    "\n",
    "        if direction == \"x\":\n",
    "            kernel = smooth_kernel_x\n",
    "        elif direction == \"y\":\n",
    "            kernel = smooth_kernel_y\n",
    "\n",
    "        return torch.abs(F.conv2d(input_tensor, kernel, padding=1))\n",
    "\n",
    "    def ave_gradient(self, input_tensor, direction):\n",
    "        return F.avg_pool2d(self.gradient(input_tensor, direction), kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def smooth(self, input_I, input_R):\n",
    "        input_R_gray = torch.mean(input_R, dim=1, keepdim=True)  # Convert to grayscale\n",
    "        return torch.mean(self.gradient(input_I, \"x\") * torch.exp(-10 * self.ave_gradient(input_R_gray, \"x\")) +\n",
    "                          self.gradient(input_I, \"y\") * torch.exp(-10 * self.ave_gradient(input_R_gray, \"y\")))\n",
    "\n",
    "    def evaluate(self, epoch_num, eval_low_data, sample_dir, train_phase):\n",
    "        print(f\"[*] Evaluating for phase {train_phase} / epoch {epoch_num}...\")\n",
    "\n",
    "        for idx, input_low_eval in enumerate(eval_low_data):\n",
    "            input_low_eval = torch.unsqueeze(input_low_eval, 0)\n",
    "\n",
    "            if train_phase == \"Decom\":\n",
    "                result_1, result_2 = self(input_low_eval, input_low_eval)[:2]\n",
    "            if train_phase == \"Relight\":\n",
    "                result_1, result_2 = self(input_low_eval, input_low_eval)[2:]\n",
    "\n",
    "            save_images(os.path.join(sample_dir, f'eval_{train_phase}_{idx + 1}_{epoch_num}.png'), result_1, result_2)\n",
    "\n",
    "    def train_model(self, train_low_data, train_high_data, eval_low_data, batch_size, patch_size, epoch, lr, sample_dir, ckpt_dir, eval_every_epoch, train_phase):\n",
    "        # Optimizer\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        print(f\"[*] Start training for phase {train_phase}.\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        image_id = 0\n",
    "\n",
    "        for epoch in range(epoch):\n",
    "            for batch_id in range(len(train_low_data) // batch_size):\n",
    "                # Generate data for a batch\n",
    "                batch_input_low = torch.zeros(batch_size, 3, patch_size, patch_size)\n",
    "                batch_input_high = torch.zeros(batch_size, 3, patch_size, patch_size)\n",
    "\n",
    "                for patch_id in range(batch_size):\n",
    "                    h, w, _ = train_low_data[image_id].shape\n",
    "                    x = random.randint(0, h - patch_size)\n",
    "                    y = random.randint(0, w - patch_size)\n",
    "\n",
    "                    rand_mode = random.randint(0, 7)\n",
    "                    batch_input_low[patch_id, :, :, :] = data_augmentation(train_low_data[image_id][x:x + patch_size, y:y + patch_size, :], rand_mode)\n",
    "                    batch_input_high[patch_id, :, :, :] = data_augmentation(train_high_data[image_id][x:x + patch_size, y:y + patch_size, :], rand_mode)\n",
    "\n",
    "                    image_id = (image_id + 1) % len(train_low_data)\n",
    "                    if image_id == 0:\n",
    "                        tmp = list(zip(train_low_data, train_high_data))\n",
    "                        random.shuffle(list(tmp))\n",
    "                        train_low_data, train_high_data = zip(*tmp)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output_R_low, output_I_low, output_I_delta, output_S, R_high, I_high_3, I_low, I_high, I_delta = self(batch_input_low, batch_input_high)\n",
    "                loss_Decom, loss_Relight = self.loss(batch_input_low, batch_input_high, output_R_low, output_I_low, output_I_delta, output_S, R_high, I_high_3,I_low, I_high, I_delta)\n",
    "\n",
    "                # Choose which loss to backpropagate depending on the phase\n",
    "                if train_phase == \"Decom\":\n",
    "                    loss_Decom.backward()\n",
    "                    optimizer.step()\n",
    "                elif train_phase == \"Relight\":\n",
    "                    loss_Relight.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                print(f\"{train_phase} Epoch: [{epoch + 1}] [{batch_id + 1}/{len(train_low_data) // batch_size}] time: {time.time() - start_time:.4f}, loss: {loss_Decom.item():.6f}\")\n",
    "\n",
    "            # Evaluate and save a checkpoint file for the model\n",
    "            if (epoch + 1) % eval_every_epoch == 0:\n",
    "                self.evaluate(epoch + 1, eval_low_data, sample_dir, train_phase)\n",
    "                self.save_checkpoint(ckpt_dir, f\"RetinexNet-{train_phase}\", epoch)\n",
    "\n",
    "    def save_checkpoint(self, ckpt_dir, model_name, epoch):\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        checkpoint_path = os.path.join(ckpt_dir, f\"{model_name}_epoch_{epoch}.pth\")\n",
    "        torch.save(self.state_dict(), checkpoint_path)\n",
    "        print(f\"[*] Saving model {model_name} at {checkpoint_path}\")\n",
    "        \n",
    "    def test(self, test_low_data, test_high_data, test_low_data_names, save_dir, decom_flag, device='cuda'):\n",
    "    # Set model to evaluation mode and move it to the appropriate device\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "\n",
    "        # Ensure save directory exists\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        print(\"[*] Testing...\")\n",
    "\n",
    "        for idx in range(len(test_low_data)):\n",
    "            print(test_low_data_names[idx])\n",
    "\n",
    "            # Extract file name and extension\n",
    "            _, name = os.path.split(test_low_data_names[idx])\n",
    "            suffix = name[name.find('.') + 1:]\n",
    "            name = name[:name.find('.')]\n",
    "\n",
    "            # Prepare input data (assuming test_low_data is a numpy array, convert it to PyTorch tensor)\n",
    "            input_low_test = torch.from_numpy(np.expand_dims(test_low_data[idx], axis=0)).float().to(device)\n",
    "\n",
    "            # Forward pass through the model (no gradient calculation required during testing)\n",
    "            with torch.no_grad():\n",
    "                R_low, I_low, I_delta, S = self(input_low_test, input_low_test)\n",
    "\n",
    "            # Convert outputs back to CPU numpy arrays for saving\n",
    "            R_low = R_low.cpu().numpy().squeeze(0)\n",
    "            I_low = I_low.cpu().numpy().squeeze(0)\n",
    "            I_delta = I_delta.cpu().numpy().squeeze(0)\n",
    "            S = S.cpu().numpy().squeeze(0)\n",
    "\n",
    "            # Save the outputs\n",
    "            if decom_flag == 1:\n",
    "                save_images(os.path.join(save_dir, f\"{name}_R_low.{suffix}\"), R_low)\n",
    "                save_images(os.path.join(save_dir, f\"{name}_I_low.{suffix}\"), I_low)\n",
    "                save_images(os.path.join(save_dir, f\"{name}_I_delta.{suffix}\"), I_delta)\n",
    "            \n",
    "            save_images(os.path.join(save_dir, f\"{name}_S.{suffix}\"), S)\n",
    "        \n",
    "        print(f\"[*] Testing complete. Images saved in {save_dir}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
